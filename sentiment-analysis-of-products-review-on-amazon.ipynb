{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# To Do\n* Make sure all variables are fed to functions as input, they don't use global variables\n* Add more clf","metadata":{}},{"cell_type":"markdown","source":"# Text Analytics: Sentiment Analysis of Products Review on Amazon\n## Processing pipeline:\n1. **Text wrangling**\n 1. Removing rows containing NA/NAN values;\n 1. Removes non-word text tokens (HTML markup, emoticons, ...);\n 1. Stem of text tokens;\n 1. Removing stop-words text tokens;\n 1. training-test split:\n     * all optimisations are done on the training set while the final optimal model is tested with the test set.\n1. **N-class scenarios (classification tasks) extraction**\n * 2-class\n * 3-class\n * 4-class\n * 5-class\n1.  **Hyperparameters optimisation**\n 1. Model (pipeline: TF-IDF -> SMOTE -> classifier) construction;\n 1. GridSearchCV is applied to find the classifier's optimal hyper-parameters; \n1. **Optimal Classifier**\n 1. The hyperparameters optimisation is repetead for following classifiers:\n     * Logistic Regression (LR);\n     * Ridge Classifier;\n     * K-Nearest Neighbors (KNN);\n     * Support Vector Machine (SVM);\n     * [Naive Bayes (NB)](https://scikit-learn.org/stable/modules/naive_bayes.html)\n     * Bagged Decision Trees (Bagging);\n     * Random Forest (RF);\n     * Gradient Boosting Machine (GBM).    \n 1. The optimal fine-tuned classifier (per scenario) maximising the grid-search average cross validation accuracy is selected.\n1. **Optimal scenario**\n * The optimal scenario maximising the grid-search average cross validation accuracy is selected.\n1. **Optimal threshold**\n * The threshold maximising the G-mean of sensitivity and specificity on the macro-averaged ROC curve (averaged across N*K-fold Cross Validation repetitions) is selected.\n1. **Optimal model**\n * The fine-tuned model optimised on the Hyperparameters, classifiers, scenarios, and thresholds is used as a final model.\n * The optimal model performance is evaluated by the (one-vs-rest & multiclass) confusion matrix and its derivative metrics:\n     * True Positive Rate (TPR), Positive Predictive Value (PPV), F1 score, and Accuracy (ACC).\n     \n**I hope you find this kernel useful and your UPVOTES would be highly appreciated.**\n     \n\n","metadata":{}},{"cell_type":"markdown","source":"## Table of Contents (ToC) <a class=\"anchor\" id=\"TOC\"></a>\n* [Introduction](#Intro)\n * [Libraries](#Libraries)\n * [Dataset](#Dataset)\n * [Data size](#DataSize)\n* [Text wrangling](#TextWrangling)\n * [Word cloud before and after text wrangling](#WordCloud)\n * [Results](#TextWranglingResults)\n* [Train-test split](#TrainTest)\n* [N-class scenarios extraction](#NClassScenarios)\n * [Class count plot per N-class scenarios](#ClassCountPlot)\n * [Results](#NClassScenariosResults)\n* [Fine-tuned optimal classifier using GridSearchCV](#optimalClassifier)\n * [GridSearchCV accuracy comparison across multi-class, i.e., 2-class, ..., 5-class, scenarios](#optimalScenario)\n * [Results](#optimalClassifierResults)\n* [Receiver Operating Characteristic (ROC) curves](#ROC)\n * [Macro-averaged ROC comparison across multi-class scenarios, i.e., 2-class, ..., 5-class](#MacroAveragedROC)\n * [Results](#ROCResults)\n* [Optimal model](#OptimalModel)\n * [Results](#OptimalModelResults)\n* [References](#References)","metadata":{}},{"cell_type":"markdown","source":"# Introduction ([ToC](#TOC))<a class=\"anchor\" id=\"Libraries\"></a>","metadata":{}},{"cell_type":"markdown","source":"## Libraries ([ToC](#TOC))<a class=\"anchor\" id=\"Libraries\"></a>","metadata":{}},{"cell_type":"code","source":"# DataFrame\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# plotting\nimport seaborn as sns # for statistical data visualization\nimport sys\nimport wordcloud # pip install wordcloud\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt # for data visualization purposes\n\n# nltk\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nnltk.download('stopwords')\n\n# sklearn\nfrom sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, GridSearchCV #, cross_val_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.preprocessing import LabelBinarizer # label_binarize\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# scipy\nfrom scipy import interp\n\n# imblearn\nfrom imblearn.over_sampling import SMOTE # conda install -c conda-forge imbalanced-learn\nfrom imblearn.pipeline import make_pipeline, Pipeline\n\n# Utility\nimport math\nimport numpy as np # linear algebra\nimport re\nimport string\nimport pickle\nfrom collections import Counter, defaultdict, Sequence\nfrom itertools import cycle, combinations, chain\nfrom IPython.display import clear_output, display_markdown\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline ","metadata":{"execution":{"iopub.status.busy":"2023-02-22T14:29:24.129013Z","iopub.execute_input":"2023-02-22T14:29:24.129421Z","iopub.status.idle":"2023-02-22T14:29:24.146323Z","shell.execute_reply.started":"2023-02-22T14:29:24.129387Z","shell.execute_reply":"2023-02-22T14:29:24.145029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset ([ToC](#TOC))<a class=\"anchor\" id=\"Dataset\"></a>\n[About dataset](https://www.kaggle.com/datasets/nehaprabhavalkar/indian-products-on-amazon)","metadata":{}},{"cell_type":"code","source":"dataDir = '../input/indian-products-on-amazon/amazon_vfl_reviews.csv'\nreviewColName = 'review' # Better programming practice: The name of review column is stored in a variable, so that in other dataset with different column name it is only enough to change this variable\nratingColName = 'rating' # Better programming practice:\nencoding = 'latin' # encoding={\"UTF-8\", 'latin'}\n# XRaw = pd.read_csv(dataDir, usecols = [reviewColName] ,encoding=encoding, squeeze = True) # raw text reviews\n# y = pd.read_csv(dataDir, usecols = [ratingColName] ,encoding=encoding, squeeze = True) # labels: reviews ratings\nXRaw_y = pd.read_csv(dataDir, usecols = [reviewColName, ratingColName] ,encoding=encoding) # raw text reviews\n\nXRaw_y.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-22T14:55:42.189513Z","iopub.execute_input":"2023-02-22T14:55:42.189933Z","iopub.status.idle":"2023-02-22T14:55:42.221568Z","shell.execute_reply.started":"2023-02-22T14:55:42.189897Z","shell.execute_reply":"2023-02-22T14:55:42.220205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data size ([ToC](#TOC))<a class=\"anchor\" id=\"DataSize\"></a>","metadata":{}},{"cell_type":"code","source":"ratingOrigColName = ratingColName+'Orig' # After merging labels, this variable name holds the original labels\nclfParamName = 'clf' # common word used in param_grid & Pipeline\ntest_size = 0.25 # test data set split fraction\nn_splits = 2 #4 Fold number in K-fold CV\nn_repeats = 2 #10: Number of repetitions of cross-validation\nrandom_state = 0 # for reproducibility\n# print('Rows nb: {}\\nColumns nb: {}\\nColumn names: {}'.format(data.shape[0], data.shape[1], (', ').join(list(data.columns)))) # Displays the size of the data\nprint('Number of observations: {}'.format(XRaw_y.shape[0])) # Displays the size of the data","metadata":{"execution":{"iopub.status.busy":"2023-02-22T14:55:46.557223Z","iopub.execute_input":"2023-02-22T14:55:46.557616Z","iopub.status.idle":"2023-02-22T14:55:46.565422Z","shell.execute_reply.started":"2023-02-22T14:55:46.557585Z","shell.execute_reply":"2023-02-22T14:55:46.564079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text wrangling ([ToC](#TOC))<a class=\"anchor\" id=\"TextWrangling\"></a>","metadata":{}},{"cell_type":"code","source":"def na_remover(dfIn):\n    \"\"\"Removes rows containing NA/NAN values\"\"\"\n    dfOut = dfIn.dropna()\n#     nanRemovedNb = dfIn.shape[0] - dfOut.shape[0] # Number of removed rows containing nan value\n#     return dfOut, nanRemovedNb\n    return dfOut\n\n\ndef nonWord_remover(textIn):\n    \"\"\"Removes non-word text tokens\"\"\"\n    textOut = re.sub('<[^>]*>', '', textIn) # Removes all of the HTML markup from the reviews.\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', textOut) # Finds emoticons\n#     textOut = re.sub('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', '', textOut) # Removes emoticons\n    textOut = (re.sub('[\\W]+', ' ', textOut.lower()) + ' '.join(emoticons).replace('-', '')) # Removes all non-word characters from the text via the regex [\\W]+ and converts the text into lowercase characters.\n    return textOut\n\n\nporter = PorterStemmer()\ndef word_stemmer(textIn):\n    \"\"\"Stem of text tokens\"\"\"\n    textOut = [porter.stem(word) for word in textIn.split()]\n    return textOut\n\n\ndef stopWords_remover(textIn):\n    \"\"\"Removes stop-words text tokens\"\"\"\n    stopWords = stopwords.words('english')+['product', 'thi', 'à', 'the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from']\n    textOut = ' '.join([word for word in textIn if word not in stopWords])\n    return textOut\n\n\ndef mostCommonWords_finder(df, N, lenMin):\n    \"\"\"Finds the most common N words of minimum length of lenMin\"\"\"\n    all_words = []\n    for line in list(df):\n        words = line.split()\n        for word in words:\n          if (len(word) > lenMin):\n            all_words.append(word.lower()) \n    return Counter(all_words).most_common(N)\n\n\ndef word_counter(df_text):\n    \"\"\"Counts the number of text tokens\"\"\"\n    return sum(df_text.apply(lambda x:len(str(x).split())))\n\n\ndef wrangler(dfIn, textColumn):\n    \"\"\"Removes NA/NAN values & non-word & stop-words text tokens\"\"\"\n#     dfOut, nanRemovedNb = na_remover(dfIn) # Removes rows containing NA/NAN values\n    dfOut = na_remover(dfIn) # Removes rows containing NA/NAN values\n    nanFilter = word_counter(dfOut[textColumn]) # Total number of text tokens after removing nan rows\n    dfOut[textColumn] = dfOut[textColumn].apply(nonWord_remover) # Removes non-word text tokens\n    nonWordFilter = word_counter(dfOut[textColumn]) # Total number of text tokens after removing non-word text tokens\n#     nonWordsRemovedNb = wordsTot2 - wordsTot1 # Total number of removed non-word text tokens\n    dfOut[textColumn] = dfOut[textColumn].apply(word_stemmer) # Stem of text tokens\n    dfOut[textColumn] = dfOut[textColumn].apply(stopWords_remover) # Removes stop-word text tokens\n#     stopWordsRemovedNb = wordsTot2-word_counter(dfOut[textColumn]) # Total number of removed stop-word text tokens\n    stopWordFilter = word_counter(dfOut[textColumn]) # Total number of text tokens after removing stop-word text tokens\n#     return dfOut, nanRemovedNb, nonWordsRemovedNb, stopWordsRemovedNb\n    return dfOut, nanFilter, nonWordFilter, stopWordFilter\n\n\ndef wordcloud_plotter(ax, data, title = None):\n    wordcloud = WordCloud(\n        background_color = 'white',\n        max_words = 200,\n        max_font_size = 40, \n        scale = 3,\n        random_state = 42\n    ).generate(str(data))\n    ax.axis('off')\n    if title: \n        ax.set_title(title, fontsize = 20)\n    ax.imshow(wordcloud)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T15:30:17.416318Z","iopub.execute_input":"2023-02-22T15:30:17.416718Z","iopub.status.idle":"2023-02-22T15:30:17.435843Z","shell.execute_reply.started":"2023-02-22T15:30:17.416683Z","shell.execute_reply":"2023-02-22T15:30:17.434303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word cloud before and after text wrangling ([ToC](#TOC))<a class=\"anchor\" id=\"WordCloud\"></a>","metadata":{}},{"cell_type":"code","source":"# dataCleaned, nanRemovedNb, nonWordsRemovedNb, stopWordsRemovedNb = wrangler(data, reviewColName)\nX_y, nanFilter, nonWordFilter, stopWordFilter = wrangler(XRaw_y, reviewColName)\nfig, axs = plt.subplots(1, 2, figsize = (20, 5)) \nfig.suptitle('Word cloud', fontsize= 30, fontweight='bold')\nwordcloud_plotter(axs[0], XRaw_y[reviewColName], 'Before cleaning')\nwordcloud_plotter(axs[1], X_y[reviewColName], 'After cleaning')\nplt.savefig(\"../working/wordCloud.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results ([ToC](#TOC))<a class=\"anchor\" id=\"TextWranglingResults\"></a>","metadata":{}},{"cell_type":"code","source":"display_markdown('''\n* Wrangling pipeline:\n 1. Removing rows containing NA/NAN values;\n 1. Removes non-word text tokens (HTML markup, emoticons, ...);\n 1. Stem of text tokens;\n 1. Removing stop-words text tokens.\n* In total, the number of removed text tokens in each wrangling step is shown below:\n * | |Nb of non-NaN rows |Non-word filtering (nb of words)|Stop-word filtering (nb of words)|\n   |:---|:---|:----|:----|\n   |Before wrangling| **{0}**  | **{3}**| **{6}**|\n   |After wrangling | **{1}**  | **{4}**| **{7}**|\n   |Before - After          | **{2}**  | **{5}**| **{8}**|\n'''.format(XRaw_y.shape[0], X_y.shape[0],XRaw_y.shape[0]-X_y.shape[0],\n           nanFilter, nonWordFilter, nanFilter-nonWordFilter,\n          nonWordFilter,stopWordFilter,nonWordFilter-stopWordFilter), raw=True) # .format(nanRemovedNb,nonWordsRemovedNb,stopWordsRemovedNb), raw=True)\ndel XRaw_y","metadata":{"execution":{"iopub.status.busy":"2023-02-22T15:35:48.437876Z","iopub.execute_input":"2023-02-22T15:35:48.438301Z","iopub.status.idle":"2023-02-22T15:35:48.461797Z","shell.execute_reply.started":"2023-02-22T15:35:48.438266Z","shell.execute_reply":"2023-02-22T15:35:48.460070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train-test split ([ToC](#TOC))<a class=\"anchor\" id=\"TrainTest\"></a>","metadata":{}},{"cell_type":"code","source":"# dataCleaned, dataCleaned_test, _, _ = train_test_split(dataCleaned, dataCleaned[ratingColName], test_size=test_size,                                                     \n#                                                     stratify = dataCleaned[ratingColName], # To make sure the splits & the whole data class proportions are roughly identical\n#                                                     random_state=random_state) # train/test split\nX, X_test, y, y_test = train_test_split(X_y[reviewColName], X_y[ratingColName], test_size=test_size,                                                     \n                                                    stratify = X_y[ratingColName], # To make sure the splits & the whole data class proportions are roughly identical\n                                                    random_state=random_state) # train/test split\ndel X_y, reviewColName, ratingColName","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# N-class scenarios extraction ([ToC](#TOC))<a class=\"anchor\" id=\"NClassScenarios\"></a>\n","metadata":{}},{"cell_type":"code","source":"def class_chooser_helper(tmp, tmp2, clsNb = None):\n    \"\"\"A helper for class_chooser\"\"\"\n    if clsNb != None:\n        jj = clsNb\n    elif clsNb == None:\n        jj = range(2, len(tmp)+1)\n    for j in jj:  \n        tmp3 = list(combinations(tmp, j))\n        for tmp4 in tmp3:\n            tmp2.append(tmp4)\n    return tmp2\n  \n    \ndef class_chooser(classLabel, clsNb = None):\n    \"\"\"Chooses possible combination of classes\"\"\"\n    tmp2 = []\n    tmp2 = class_chooser_helper(classLabel, tmp2, clsNb)\n    for k in range(2,len(classLabel)):\n        for i in range(0,len(classLabel)-k+1):\n            tmp = []\n            tmp.append((classLabel[i:i+k]))\n            rest = set(classLabel)-set(classLabel[i:i+k])\n            for tmp5 in rest:\n                tmp.append(tmp5)\n            tmp2 = class_chooser_helper(tmp, tmp2, clsNb)\n    tmp2 = set(tmp2)\n    classNb = defaultdict(list)\n    for tmp in tmp2:\n        classNb[len(tmp)].append(tmp)\n    return classNb\n\n\ndef ismember(a, b): # https://devpress.csdn.net/python/630459aac67703293080ba45.html\n    \"\"\"Equivalent to MATLAB's ismember function\"\"\"\n    bind = {}\n    for i, elt in enumerate(b):\n        if elt not in bind:\n            bind[elt] = True\n    return [bind.get(itm, False) for itm in a]  # None can be replaced by any other \"not in b\" value\n\n\n# def class_merger(dataCleaned, ratingColName, classes):\ndef class_merger(X, y, classes):\n    addVal = 100 \n    flattenedClass = []\n    for i in classes:\n        if isinstance(i, (Sequence, np.ndarray)): # https://stackoverflow.com/questions/16807011/python-how-to-identify-if-a-variable-is-an-array-or-a-scalar\n            for j in i:\n                flattenedClass.append(j)\n        else:\n            flattenedClass.append(i)\n    selectedClassesIdx = ismember(y, flattenedClass)\n#     dataCleanedMerged = dataCleaned.loc[selectedClassesIdx, :]\n#     print(len(y), len(selectedClassesIdx))\n    if X.empty:\n        XMerged = None\n    else:\n        XMerged = X.iloc[selectedClassesIdx]\n    yMerged, yOrig = y.iloc[selectedClassesIdx], y.iloc[selectedClassesIdx]\n#     dataCleanedMerged[ratingOrigColName] = dataCleaned[ratingColName]\n#     yOrig = y.iloc[selectedClassesIdx]\n#     print(yMerged)\n    for newClass, oldClass in enumerate(classes):\n#         yMerged = yMerged.replace({ratingColName:oldClass}, newClass + addVal)\n#         yMerged = yMerged.replace({ratingOrigColName:oldClass}, str(oldClass))\n        yMerged = yMerged.replace(oldClass, newClass + addVal)\n        yOrig = yOrig.replace(oldClass, str(oldClass))\n#     yMerged[ratingColName] -= addVal\n    yMerged -= addVal\n    return XMerged, yMerged, yOrig\n\n\ndef nRowCol_calculator(subplotsNb, subplotsCols = 5):\n    if len(subplotsNb) > subplotsCols:\n        nCol = subplotsCols\n        nRow = math.ceil(len(subplotsNb) / nCol)\n    else:\n        nCol = len(subplotsNb)\n        nRow = 1 \n    return nRow, nCol\n\n\ndef plot_parameters(axs, pltIdx, nRow, nCol):\n    rows, cols = divmod(pltIdx, nCol) # https://stackoverflow.com/questions/28995146/matlab-ind2sub-equivalent-in-python\n    if nRow != 1:\n            ax = axs[rows, cols]\n    elif nCol != 1:\n        ax = axs[cols]\n    elif nCol == 1:\n        ax = axs    \n    if rows+1 == nRow:\n        isXlabel = True\n    else:\n        isXlabel = False\n    if cols+1 == 1:\n        isYlabel = True\n    else:\n        isYlabel = False\n    return ax, isXlabel, isYlabel\n\n\n# def classSize_plotter(ax, isXlabel, isYlabel, df, df_col, df_colOrig, colors_classes):\ndef classSize_plotter(ax, isXlabel, isYlabel, y, yOrig, colors_classes):\n    df = pd.concat([y, yOrig], axis=1)\n    df.columns = ['y', 'yOrig']    \n#     sns.countplot(ax=ax, x = df_col, hue=df_colOrig, data = df, palette=colors_classes, dodge=False)\n    sns.countplot(ax=ax, x = 'y', hue='yOrig', data = df, palette=colors_classes, dodge=False)\n    if not isXlabel:\n        ax.set_xlabel('')\n    if not isYlabel:\n        ax.set_ylabel('')\n\n    \n# def classSizeTot_plotter(classNb, classNbIdx, dataCleaned, ratingColName, colors_classes):\ndef classSizeTot_plotter(classNb, classNbIdx, y, colors_classes):\n    nRow, nCol = nRowCol_calculator(classNb[classNbIdx])    \n    fig, axs = plt.subplots(nRow, nCol, figsize = (20, 4*nRow)) \n    fig.suptitle('Class size for {}-class classification task'.format(classNbIdx), fontsize= 30, fontweight='bold')\n    for pltIdx, classes in enumerate(classNb[classNbIdx]): # Scenarios\n#         dataCleanedMerged = class_merger(dataCleaned, ratingColName, classes)\n        _, yMerged, yOrig = class_merger(pd.Series([]), y, classes) # No need to X\n        ax, isXlabel, isYlabel = plot_parameters(axs, pltIdx, nRow, nCol)\n#         classSize_plotter(ax, isXlabel, isYlabel, dataCleanedMerged, ratingColName, ratingOrigColName, colors_classes)\n        classSize_plotter(ax, isXlabel, isYlabel, yMerged, yOrig, colors_classes)\n    plt.savefig(\"../working/{}-classSize.png\".format(classNbIdx))\n    \n\ndef scenario_counter(classNb):\n    scenarioNb = []\n    for val in classNb.values():\n        scenarioNb.append(len(val))\n    return scenarioNb","metadata":{"execution":{"iopub.status.busy":"2023-02-22T16:57:03.134087Z","iopub.execute_input":"2023-02-22T16:57:03.134635Z","iopub.status.idle":"2023-02-22T16:57:03.167304Z","shell.execute_reply.started":"2023-02-22T16:57:03.134579Z","shell.execute_reply":"2023-02-22T16:57:03.165576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Class count plot per N-class scenarios ([ToC](#TOC))<a class=\"anchor\" id=\"ClassCountPlot\"></a>\n* As it can be seen in the next figure, in some scenarios the number of observations per class can be highly imbalanced/skewed, which can lead to unreliable results if it is not treated.\n* [Imbalanced-learn](https://imbalanced-learn.org/stable/) is a scikit-learn-based library that provides some useful tools (among which [SMOTE](https://www.jair.org/index.php/jair/article/view/10302) is selected here in the processing pipeline) to deal with imbalanced datasets. ","metadata":{}},{"cell_type":"code","source":"colors_classes = sns.color_palette()\n# classLabel = tuple(np.unique(dataCleaned[ratingColName]))\nclassLabel = tuple(np.unique(y))\nclassNb = class_chooser(classLabel, clsNb = None) # Specify clsNb=None, for general case [4, 5]\nnClass = list(classNb.keys())\nscenarioNb = scenario_counter(classNb)\nfor classNbIdx in classNb.keys():\n#     classSizeTot_plotter(classNb, classNbIdx, dataCleaned, ratingColName, colors_classes)\n    classSizeTot_plotter(classNb, classNbIdx, y, colors_classes)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T16:57:05.247088Z","iopub.execute_input":"2023-02-22T16:57:05.247703Z","iopub.status.idle":"2023-02-22T16:57:19.023853Z","shell.execute_reply.started":"2023-02-22T16:57:05.247666Z","shell.execute_reply":"2023-02-22T16:57:19.022486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results ([ToC](#TOC))<a class=\"anchor\" id=\"NClassScenariosResults\"></a>","metadata":{}},{"cell_type":"code","source":"display_markdown('''\n* For each total number of N classes, i.e., 2, 3, 4, and 5, all possible N selection of classes from the original number of classes (5) has been considered:\n * E.g., for N = 3 selection out of 5, some 3-class scenarios are:\n      * [1, 2, 3], \n      * [1, 2, 4], \n      * [1, 2, 5], ...\n* Also all possible merging of **neighboring** classes are considered:\n * E.g., for N = 3, some merged 3-class scenarios are (merged classes are enclosed in parentheses):\n     * [(1,2), 3, 4], \n     * [1, (2,3), 4], \n     * [1, 2, (3,4)],\n     * [(1,2,3), 4, 5],\n     * [1,(2,3,4), 5],\n     * [1, 2, (3,4,5)].\n* In total, with 5 initial classes there are the following number of N-class scenarios per N:\n * | N-class |Nb of scenarios|\n   |:---|----:|\n   | **{0}**  | **{4}**|\n   | **{1}**  | **{5}**|\n   | **{2}**  | **{6}**|\n   | **{3}**  | **{7}**|\n'''.format(nClass[0],nClass[1],nClass[2],nClass[3],\n          scenarioNb[0], scenarioNb[1], scenarioNb[2], scenarioNb[3]), raw=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T16:48:56.951590Z","iopub.execute_input":"2023-02-22T16:48:56.952044Z","iopub.status.idle":"2023-02-22T16:48:56.960986Z","shell.execute_reply.started":"2023-02-22T16:48:56.952007Z","shell.execute_reply":"2023-02-22T16:48:56.959734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine-tuned optimal classifier using GridSearchCV ([ToC](#TOC))<a class=\"anchor\" id=\"optimalClassifier\"></a>\n* Exhaustive Grid Search ([sklearn.model_selection.GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV))\n* Classifiers:\n * [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html):\n     * “naive”/\"idiot\" assumption: conditional independence between every pair of features given the value of the class variable:\n         * curse of dimensionality-related problems are alleviated, because each feature distribution can be independently and individually (1D feature space) estimated\n     * Maximum A Posteriori (MAP) estimation to estimate class label;\n     * [It works quite well in many real-world situations: document classification, spam filtering](https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf);\n     * A small amount of training data is required\n     * Extremely fast compared to more sophisticated methods\n     * The probability outputs from predict_proba() are not reliable, so cannot be interpreted as a confidence level ([bad estimator](https://scikit-learn.org/stable/auto_examples/calibration/plot_compare_calibration.html#sphx-glr-auto-examples-calibration-plot-compare-calibration-py)).\n     * Variants:\n         * Gaussian Naive Bayes ([sklearn.naive_bayes.GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)):\n             * The likelihood of the features is assumed to be Gaussian;\n             * The Gaussian parameters are estimated using maximum likelihood.\n         * Multinomial Naive Bayes ([sklearn.naive_bayes.MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB))\n             * The likelihood of the features is assumed to be multinomial;\n             * Suitable for classification with discrete features (e.g., word counts for text classification), but in practice fractional counts such as tf-idf may also work.\n             * The multinomial parameters are estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting.\n             \n             \n             \n([sklearn.svm.SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC))\n\n([sklearn.gaussian_process.GaussianProcessClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier))\n\n([sklearn.tree.DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier))\n\n([sklearn.neural_network.MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier))\n\n([sklearn.ensemble.AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier))\n     \n([sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis))","metadata":{}},{"cell_type":"code","source":"def tfidf_instantiator(): \n    TfidfVectorizerParams = {'sublinear_tf':True, 'strip_accents':None, 'lowercase':False, 'preprocessor':None} # To meke sure the same TfidfVectorizer params are used across the code\n    tfidf  = TfidfVectorizer(**TfidfVectorizerParams) \n    return tfidf\n\n\ndef smote_instantiator(y): \n    \"\"\"SMOTE value is equal to the maximum class size\"\"\"\n    smoteClassSize = pd.Series(np.asarray(y)).value_counts(sort=True).iloc[0] # The resulting object will be in descending order so that the first element is the most frequently-occurring element.\n    sampling_strategy = {}\n    for i in np.unique(y):\n        sampling_strategy[i] = smoteClassSize\n    smote = SMOTE(random_state=random_state, sampling_strategy=sampling_strategy ) #  , k_neighbors = 3\n    return smote\n\n\ndef clf_instantiator(clfName):     \n    if clfName == \"svm\":\n        clf = svm.SVC(probability=True, random_state=random_state) \n        param_grid = [{clfParamName+'__'+'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], # linear\n                           clfParamName+'__'+'C': [50, 10, 1.0, 0.1, 0.01],\n                           clfParamName+'__'+'gamma': ['scale', 'auto']}]\n    elif clfName == \"knn\":\n        clf = KNeighborsClassifier() \n        param_grid = [{clfParamName+'__'+'n_neighbors': range(1, 5, 2)}]#, range(1, 21, 2)\n#                            clfParamName+'__'+'weights': ['uniform', 'distance'],\n#                            clfParamName+'__'+'metric': ['euclidean', 'manhattan', 'minkowski', 'canberra']}]\n    elif clfName == \"lr\":\n        clf = LogisticRegression(random_state=random_state) \n        param_grid = [{clfParamName+'__'+'solver': ['newton-cg', 'lbfgs', 'liblinear'],\n            clfParamName+'__'+'penalty': ['l1', 'l2'], # ['l1', 'l2']\n                       clfParamName+'__'+'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}] # [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n    elif clfName == \"ridge\":\n        clf = RidgeClassifier(random_state=random_state) \n        param_grid = [{clfParamName+'__'+'alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]}] \n    elif clfName == \"gp\":\n        clf = GaussianProcessClassifier(random_state=random_state) \n        param_grid = \"?\"\n    elif clfName == \"gnb\":\n        clf = GaussianNB() \n        param_grid = \"?\"     \n    elif clfName == \"mnb\":\n        clf = MultinomialNB() \n        param_grid = \"?\"             \n    elif clfName == \"dt\":\n        clf = DecisionTreeClassifier(random_state=random_state)        \n        param_grid = [{clfParamName+'__'+'max_features': ['sqrt', 'log2']+list(range(0,14)),\n                      clfParamName+'__'+'max_depth': list(range(10,15))}] \n    elif clfName == \"bagging\":\n        clf = BaggingClassifier(random_state=random_state) \n        param_grid = [{clfParamName+'__'+'n_estimators': [10, 100, 1000]}] \n    elif clfName == \"rforest\":\n        clf = RandomForestClassifier(random_state=random_state) \n        param_grid = [{clfParamName+'__'+'n_estimators': [10, 100, 1000],\n                      clfParamName+'__'+'max_features': ['sqrt', 'log2']+list(range(0,14)),\n                      clfParamName+'__'+'max_depth': list(range(10,15))}] \n    elif clfName == \"gbm\":\n        clf = GradientBoostingClassifier(random_state=random_state) \n        param_grid = [{clfParamName+'__'+'n_estimators': [10, 100, 1000],\n                      clfParamName+'__'+'learning_rate': [0.001, 0.01, 0.1],\n                      clfParamName+'__'+'subsample': [0.5, 0.7, 1.0],\n                      clfParamName+'__'+'max_depth': [3, 7, 9]}]\n    elif clfName == \"mlp\":\n        clf = MLPClassifier(random_state=random_state) \n        param_grid = \"?\"\n    return clf, param_grid\n\n\ndef model_builder(y, clfName, clfParamName):    \n    tfidf = tfidf_instantiator()        \n    smote = smote_instantiator(y)\n    if type(clfName) == str: # Grid-search mode: instantiate clf\n        clf, param = clf_instantiator(clfName)\n    else: # uses previously-computed optimal clf\n        clf, param = clfName, None\n    model = Pipeline([('tfidf', tfidf), (clfParamName, clf)]) # , ('smote', smote)\n    return model, param\n\n\ndef params_counter(param):\n    paramCount = 1\n    for val in param[0].values():\n        paramCount *= len(val)\n    return paramCount\n \n\ndef optClf_finder(clfNames, X, y):\n    \"\"\"Finds optimal clf and related params using gridSearchCV\"\"\"\n    gridSearchCV = {}\n    clfParamNb = {}\n    for clfName in clfNames:\n        model, param = model_builder(y, clfName, clfParamName)\n        clfParamNb[clfName] = params_counter(param)\n        gs = GridSearchCV(model, param, scoring='accuracy', cv=n_splits, verbose=2, n_jobs=-1)\n        gs.fit(X, y)\n        gridSearchCV[clfName] = [gs.best_params_, gs.best_score_]\n        print('Best parameter set: {}'.format(gs.best_params_))\n        print('CV Accuracy: {}'.format(gs.best_score_))\n    acc = []\n    for val in gridSearchCV.values():\n        acc.append(val[1])\n    optClfAcc = np.max(acc)\n    optClfName = clfNames[np.argmax(acc)] # Index of classifier with maximum performance in grid search\n    optParam = gridSearchCV[optClfName][0]\n    optParam2 = {}\n    for key, val in optParam.items():\n        optParam2[key[len(clfParamName)+2:]] = val\n    optClf, _ = clf_instantiator(optClfName) # instantiate the optimal clf\n    optClf.set_params(**optParam2) # set the optimal params\n    print('optimal clf is {}.'.format(optClfName))\n    clear_output(wait=False)\n    return optClf, optClfName, optClfAcc, clfParamNb\n\n\ndef gridSearch_plotter(optClasses, optClfsName, optClfsAcc, classNbIdx):\n    xtickLabel = []\n    for i, j in zip(optClasses, optClfsName):\n        xtickLabel.append(str(i) + '-' + j)  \n    sorteds = list(zip(xtickLabel, optClfsAcc)) # sort acc\n    sorteds.sort(key = lambda x: x[1])\n    xtickLabel, optClfsAcc = list(zip(*sorteds))[0], list(zip(*sorteds))[1]\n    fig, _ = plt.subplots(1, 1, figsize = (20, 4)) \n    fig.suptitle('Grid search accuracy (cv={}) for {}-class classification tasks'.format(n_splits, classNbIdx), fontsize= 30, fontweight='bold')\n#     plt.plot(optClfsAcc, 'o-')\n    plt.stem(optClfsAcc)\n    plt.xticks(np.arange(len(optClfsAcc)), xtickLabel, rotation=30)\n    plt.ylabel('Accuracy')\n    plt.xlabel('Optimum classifier per class merging')\n    plt.grid()\n    plt.savefig(\"../working/{}-GridSearchAcc.png\".format(classNbIdx)) ","metadata":{"execution":{"iopub.status.busy":"2023-02-22T16:49:44.194876Z","iopub.execute_input":"2023-02-22T16:49:44.195318Z","iopub.status.idle":"2023-02-22T16:49:44.230001Z","shell.execute_reply.started":"2023-02-22T16:49:44.195274Z","shell.execute_reply":"2023-02-22T16:49:44.228654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clfNames = ['knn'] # ['svm', 'ridge', 'bagging', 'rforest', 'gbm', 'knn', 'lr']\noptClfs = defaultdict(list)\noptClfsName = defaultdict(list)\noptClasses = defaultdict(list)\noptClfsAcc = defaultdict(list)\ndataCleanedMerged = defaultdict(list)\nfor classNbIdx in classNb.keys(): # N-classes\n    for classes in classNb[classNbIdx]: # Scenarios\n#         dataMerged = class_merger(dataCleaned, ratingColName, classes)\n        XMergedTmp, yMergedTmp, yMergedOrigTmp = class_merger(X, y, classes)\n#         dataCleanedMerged[classNbIdx].append(dataMerged)\n        dataCleanedMerged[classNbIdx].append((XMergedTmp, yMergedTmp, yMergedOrigTmp))\n#         optClf, optClfName, optClfAcc, clfParamNb = optClf_finder(clfNames, dataMerged[reviewColName], dataMerged[ratingColName])\n        optClf, optClfName, optClfAcc, clfParamNb = optClf_finder(clfNames, XMergedTmp, yMergedTmp)\n        optClfs[classNbIdx].append(optClf)\n        optClfsName[classNbIdx].append(optClfName)\n        optClasses[classNbIdx].append(classes)\n        optClfsAcc[classNbIdx].append(optClfAcc)  \noptDataCleanedMerged = []\noptClf = []\noptClfName = []\noptClass = []\noptClfAcc = []\n# dataCleanedMerged_test = []\nyMerged_test = []\nfor classNbIdx in classNb.keys():   \n    gridSearch_plotter(optClasses[classNbIdx], optClfsName[classNbIdx], optClfsAcc[classNbIdx], classNbIdx)    \n    optIdx = np.argmax(optClfsAcc[classNbIdx]) # Maximum acc idx across different combinations of the classNbIdx-class classification task\n    optDataCleanedMerged.append(dataCleanedMerged[classNbIdx][optIdx])\n    optClf.append(optClfs[classNbIdx][optIdx])\n    optClfName.append(optClfsName[classNbIdx][optIdx])\n    optClass.append(optClasses[classNbIdx][optIdx])\n    optClfAcc.append(optClfsAcc[classNbIdx][optIdx])    \n#     dataCleanedMerged_test.append(class_merger(dataCleaned_test, ratingColName, optClasses[classNbIdx][optIdx])) # Merge test labels based on the optimal merging\n    yMerged_test.append((class_merger(X_test, y_test, optClasses[classNbIdx][optIdx]))) # Merge test labels based on the optimal merging\nclear_output(wait=False)\ndel dataCleanedMerged, optClfs, optClfsName, optClasses, optClfsAcc, optIdx","metadata":{"execution":{"iopub.status.busy":"2023-02-22T17:09:16.181462Z","iopub.execute_input":"2023-02-22T17:09:16.181862Z","iopub.status.idle":"2023-02-22T17:09:31.939697Z","shell.execute_reply.started":"2023-02-22T17:09:16.181830Z","shell.execute_reply":"2023-02-22T17:09:31.938211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GridSearchCV accuracy comparison across multi-class, i.e., 2-class, ..., 5-class, scenarios ([ToC](#TOC))<a class=\"anchor\" id=\"optimalScenario\"></a>","metadata":{}},{"cell_type":"code","source":"gridSearch_plotter(optClass, optClfName, optClfAcc, str(list(classNb.keys())))","metadata":{"execution":{"iopub.status.busy":"2023-02-22T16:58:57.790636Z","iopub.execute_input":"2023-02-22T16:58:57.791114Z","iopub.status.idle":"2023-02-22T16:58:58.107487Z","shell.execute_reply.started":"2023-02-22T16:58:57.791072Z","shell.execute_reply":"2023-02-22T16:58:58.106324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results ([ToC](#TOC))<a class=\"anchor\" id=\"optimalClassifierResults\"></a>","metadata":{}},{"cell_type":"code","source":"display_markdown('''\n* For each N-class classification task, i.e., 2-class, ..., 5-class, and its scenarios the following procedure is repeated:\n    1. Model (pipeline: TF-IDF -> SMOTE -> classifier) is built;\n    1. GridSearchCV is applied on the follwong classifiers to find their optimal hyper-parameters:\n        * Support Vector Machines (SVM): optimisation across ? hyperparameters\n        * K-Nearest Neighbors (KNN): optimisation across **{0}** hyperparameters\n        * Logistic Regression (LR): optimisation across **{1}** hyperparameters\n        * Naive Bayes (NB): optimisation across ? hyperparameters\n    1. The optimal classifier among the above fine-tuned ones (in total **{2}** models) maximising the average cross validation accuracy is selected.\n* Then, for each N-class classification task, i.e., 2-class, ..., 5-class, the optimal scenario with maximum average cross validation accuracy is selected, as shown below:\n * | N-class |Nb of scenarios|Opt. scenario| Nb of hyperparams| Opt. classifier| GridSearchCV ACC|\n   |:---|:---|----|----:|----:|----:|\n   | **{3}**  | **{19}**  |  **{4}**|  **{2}**  |**{5}**  |  **{6}**  |\n   | **{7}**  | **{20}**  | **{8}**|  **{2}**  | **{9}**  | **{10}**  |\n   | **{11}**  | **{21}**  |  **{12}**|  **{2}**  |**{13}**  |  **{14}**  |\n   | **{15}**  | **{22}**  |  **{16}** |  **{2}** |**{17}**  |  **{18}**  |\n'''.format(clfParamNb['knn'], clfParamNb['lr'], clfParamNb['knn']+clfParamNb['lr'], \n          nClass[0], optClass[0], optClfName[0], round(optClfAcc[0], 2),\n           nClass[1], optClass[1], optClfName[1], round(optClfAcc[1], 2),\n           nClass[2], optClass[2], optClfName[2], round(optClfAcc[2], 2),\n           nClass[3], optClass[3], optClfName[3], round(optClfAcc[3], 2),\n          scenarioNb[0], scenarioNb[1], scenarioNb[2], scenarioNb[3]), raw=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T16:59:01.817237Z","iopub.execute_input":"2023-02-22T16:59:01.817608Z","iopub.status.idle":"2023-02-22T16:59:01.843114Z","shell.execute_reply.started":"2023-02-22T16:59:01.817578Z","shell.execute_reply":"2023-02-22T16:59:01.841585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Receiver Operating Characteristic ([ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)) curves ([ToC](#TOC))<a class=\"anchor\" id=\"ROC\"></a>","metadata":{}},{"cell_type":"code","source":"# def output_binariser(X, Y, classLabel):\n#     \"\"\"Binarize the output\"\"\"\n#     if len(classLabel) == 2:\n#         addedLabel = classLabel[-1]+1       \n#         classes = classLabel.copy()\n#         classes.append(addedLabel)\n#         y_binary = label_binarize(Y, classes=classes)\n#         y_binary = y_binary[:,:-1]\n#     else:\n#         y_binary = label_binarize(Y, classes=classLabel)\n#     X_binary, Y_binary = [], []\n#     for i in range(y_binary.shape[1]): \n#         Xrsmp, Yrsmp = X, y_binary[:, i]\n#         X_binary.append(Xrsmp)\n#         Y_binary.append(Yrsmp)\n#     return X_binary, Y_binary\n\n\nclass MyLabelBinarizer(LabelBinarizer): # https://stackoverflow.com/questions/31947140/sklearn-labelbinarizer-returns-vector-when-there-are-2-classes\n    def transform(self, y):\n        Y = super().transform(y)\n        if self.y_type_ == 'binary':\n            return np.hstack((Y, 1-Y))\n        else:\n            return Y\n\n    def inverse_transform(self, Y, threshold=None):\n        if self.y_type_ == 'binary':\n            return super().inverse_transform(Y[:, 0], threshold)\n        else:\n            return super().inverse_transform(Y, threshold)\n        \n        \ndef idx_finder_micro(classNb):\n    idx = []\n    for i in range(n_repeats*n_splits):\n        idx.append(np.asarray(list(range((classNb-1)*X_test.shape[0], classNb*X_test.shape[0]))) + i * (n_classes-1)*X_test.shape[0])\n    idx = np.hstack(idx)\n    return idx\n\n\ndef fpr_unique_finder(n_classes, classNb, fpr_all):\n    idx = list(range((classNb-1), n_classes * n_repeats*n_splits, n_classes))\n    fprSel = []\n#     print(idx)\n    for i in idx:\n        fprSel.append(fpr_all[i])\n    fprUnique = np.unique(np.concatenate(np.asarray(fprSel)))\n    return fprUnique\n\n\ndef microAvg_calc(n_classes, y_test_all_ravel, y_score_all):\n    # Compute micro-average ROC curve and ROC area\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()    \n    for i in range(n_classes):\n        micro_tmp = \"micro_{}\".format(i+1)\n        fpr[micro_tmp], tpr[micro_tmp], _ = roc_curve(y_test_all_ravel[idx_finder_micro(i+1)], y_score_all[idx_finder_micro(i+1)])\n        roc_auc[micro_tmp] = auc(fpr[micro_tmp], tpr[micro_tmp])  \n    \n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_all_ravel, y_score_all)\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    return fpr, tpr, roc_auc\n\n\ndef roc_bestThr(tpr, fpr, thr, fieldName):\n#     https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/\n    roc_auc = {}\n    gmeans = np.sqrt(tpr * (1-fpr)) # calculate the g-mean for each threshold\n    roc_auc[\"{}BestThrIdx\".format(fieldName)] = np.argmax(gmeans) # locate the index of the largest g-mean\n    roc_auc[\"{}BestThrVal\".format(fieldName)] = thr[roc_auc[\"{}BestThrIdx\".format(fieldName)]] \n    roc_auc[\"{}BestThrGmeans\".format(fieldName)] = gmeans[roc_auc[\"{}BestThrIdx\".format(fieldName)]]\n    return roc_auc\n\n\ndef macroAvg_calc(n_classes, n_repeats, n_splits, fpr_all, fpr, tpr, thresholds):\n    # First aggregate all false positive rates\n    fpr_unique_perClass = []\n    for i in range(n_classes):\n        fpr_unique_perClass.append(fpr_unique_finder(n_classes, i+1, fpr_all))  \n    fpr_unique = np.unique(np.concatenate(fpr_all))\n    mean_tpr_perClass = {}\n    for j in range(n_classes):\n        mean_tpr_perClass[j] = np.zeros_like(fpr_unique_perClass[j])\n    mean_tpr = np.zeros_like(fpr_unique)\n    mean_thr_perClass = {}\n    for j in range(n_classes):\n        mean_thr_perClass[j] = np.zeros_like(fpr_unique_perClass[j])\n    mean_thr = np.zeros_like(fpr_unique)\n    for i in range(n_classes):\n        for j in range(n_repeats*n_splits):\n            mean_tpr += interp(fpr_unique, fpr[i][j], tpr[i][j])\n            mean_tpr_perClass[i] += interp(fpr_unique_perClass[i], fpr[i][j], tpr[i][j])\n            mean_thr += interp(fpr_unique, fpr[i][j], thresholds[i][j])\n            mean_thr_perClass[i] += interp(fpr_unique_perClass[i], fpr[i][j], thresholds[i][j])\n    # Finally average it and compute AUC\n    mean_tpr /= (n_classes * n_repeats*n_splits)\n    mean_thr /= (n_classes * n_repeats*n_splits)\n    for i in range(n_classes):\n        mean_tpr_perClass[i] /= (n_repeats*n_splits)\n        mean_thr_perClass[i] /= (n_repeats*n_splits)\n\n    roc_auc = dict()\n    for i in range(n_classes):\n        macro_tmp = \"macro_{}\".format(i+1)                         \n        fpr[macro_tmp], tpr[macro_tmp] = np.hstack([0, fpr_unique_perClass[i], 1]), np.hstack([0, mean_tpr_perClass[i], 1])\n        \n        roc_auc_ = roc_bestThr(tpr[macro_tmp], fpr[macro_tmp], mean_thr_perClass[i], macro_tmp)\n        roc_auc.update(roc_auc_)\n    \n        roc_auc[macro_tmp] = auc(fpr[macro_tmp], tpr[macro_tmp])\n\n    fpr[\"macro\"], tpr[\"macro\"] = np.hstack([0, fpr_unique, 1]), np.hstack([0, mean_tpr, 1])\n    roc_auc_ = roc_bestThr(tpr[\"macro\"], fpr[\"macro\"], mean_thr, \"macro\")\n    roc_auc.update(roc_auc_)\n    \n    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])                                 \n    return fpr, tpr, roc_auc\n\n\ndef split_feature_clf_cv(X, y, clf, n_splits, n_repeats):\n    \"\"\"Splits into train/test; extracts features; trains & tests the classifier using N*K-fold cross-validation. Returns average test accuracy.\"\"\"\n    cv = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=random_state) \n    acc, fpr, tpr, thr, auc_, y_testTot, y_scoreTot = [], [], [], [], [], [], []\n    for train_index, test_index in cv.split(X, y): # 0<k<(n_repeats*n_splits-1)\n        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index] # train/test split\n        model, _ = model_builder(y_train, clf, '')\n        model.fit(X_train, y_train)                 \n        if len(np.unique(y)) == 2: # If binary task  \n            y_score = model.predict_proba(X_test)[:, 1]\n            fpr_, tpr_, thr_ = roc_curve(y_test, y_score) \n            auc__ = auc(fpr_, tpr_)\n        else:\n            fpr_, tpr_, thr_, auc__, y_score = np.nan, np.nan, np.nan, np.nan, np.nan\n        acc.append(model.score(X_test, y_test)) # Classifier acc         \n        fpr.append(fpr_)\n        tpr.append(tpr_)\n        thr.append(thr_)\n        auc_.append(auc__)\n        y_testTot.append(y_test)\n        y_scoreTot.append(y_score)\n    y_testTot, y_scoreTot = np.hstack(y_testTot), np.hstack(y_scoreTot)\n    return acc, fpr, tpr, thr, auc_, y_testTot, y_scoreTot\n\n\ndef negClass_finder(classLabel, classLabelTot, origNew):\n    theRest = list(set(classLabelTot) - set([classLabel]))\n    theRestTot = []\n    for i in theRest:\n        theRestTot.append(origNew[i])\n    return ','.join(theRestTot)\n\n\ndef rocRpt_plotter(ax, classLabelTot, fpr, fprMacro, tpr, tprMacro, aucMacro, \n                   aucBestThrIdx, aucBestThrGMean, color, lw, classLabel, isYlabel, origNew, ylabel):\n    for j in range(len(fpr)): \n        if j == 1:\n            ax.plot(fpr[j], tpr[j], color=color, lw=lw, label='Classes {}'.format(origNew[classLabel]), alpha=1) # \n        ax.plot(fpr[j], tpr[j], color=color, lw=lw) # , alpha=0.5\n\n    ax.plot(fprMacro, tprMacro, 'k', lw=2*lw, label='Macro-avg (AUC = {0:0.2f})'\n                 .format(aucMacro))\n    ax.scatter(fprMacro[aucBestThrIdx], tprMacro[aucBestThrIdx], marker='o', color='k', s=2**7, zorder=20,\n                    label='Opt (GM = {0:0.2f})'.format(aucBestThrGMean))  \n    theRestTot = negClass_finder(classLabel, classLabelTot, origNew)\n    rocAxis_plotter(ax, lw, ylabel, \"Pos. label : {} \\n Neg. labels : {}\".format(origNew[classLabel], theRestTot))\n\n\ndef rocAxis_plotter(ax, lw, ylabel=None, title=None):\n    ax.axis('square')\n    ax.plot([0, 1], [0, 1], 'k--', lw=0.5*lw)\n    ax.axis([-0.05, 1.0, 0.0, 1.05])\n    ax.set_ylabel(ylabel, fontsize=12)  \n    ax.set_xlabel('False Positive Rate (FPR)', fontsize=12)\n    ax.set_title (title, fontdict = {'size':12}) # , pad = 20 \n    ax.legend(loc=\"lower right\")\n    ax.grid() \n    \n    \ndef origNewLabel_finder(origLabel, newLabel): \n    newLabelUnique = np.unique(newLabel)\n    origNew = {}\n    for i in newLabelUnique:\n        origNew[i]= origLabel[newLabel == i][0]\n    return origNew\n    \n    \n# def rocTot_plotter(optDataCleanedMerged, reviewColName, ratingColName, ratingOrigColName, clfName, clf, lw, colors_classes):\ndef rocTot_plotter(optDataCleanedMerged, clfName, clf, lw, colors_classes):    \n#     YOrig = optDataCleanedMerged[ratingOrigColName].values\n#     X = optDataCleanedMerged[reviewColName].values\n#     y = optDataCleanedMerged[ratingColName].values\n    YOrig = optDataCleanedMerged[2].values\n    X = optDataCleanedMerged[0].values\n    y = optDataCleanedMerged[1].values\n#     print(y)\n#     return\n    origNew = origNewLabel_finder(YOrig, y)\n    classLabel = list(np.unique(y))   \n    \n#     X_binary, Y_binary = output_binariser(X, y, classLabel)\n    Y_binary = MyLabelBinarizer().fit_transform(y)\n#     print(range(Y_binary.shape[1]))\n#     return\n\n    n_classes = len(classLabel)  \n    fig, ax = plt.subplots(1, n_classes+1, figsize=(20,5), sharey=True, sharex=True)\n    fig.suptitle('ROC ({}-class; {}*{}-fold cross-validation)'.format(n_classes, n_repeats, n_splits), #  performance evaluation technique\n                fontsize=20, fontweight='bold')          \n    fpr_models = [] #dict()\n    acc, fpr, tpr, thr, auc_ = {}, {}, {}, {}, {}\n    y_testTot, y_scoreTot, fprTot = [], [], []\n#     for i, (X, y) in enumerate(zip(X_binary, Y_binary)):\n    for i in range(Y_binary.shape[1]): # ovr cases\n#         acc[i], fpr[i], tpr[i], thr[i], auc_[i], y_testTot_, y_scoreTot_ = split_feature_clf_cv(X, y, clf, n_splits, n_repeats)\n        acc[i], fpr[i], tpr[i], thr[i], auc_[i], y_testTot_, y_scoreTot_ = split_feature_clf_cv(X, Y_binary[:, i], clf, n_splits, n_repeats)\n        y_testTot.append(y_testTot_)\n        y_scoreTot.append(y_scoreTot_)\n        fprTot.append(fpr[i])\n#         print(len(fpr[i]))\n#     print(len(fprTot))\n    y_testTot, y_scoreTot, fprTot = np.hstack(y_testTot), np.hstack(y_scoreTot), np.vstack(fprTot)\n    print(len(fprTot))\n    fpr_macro, tpr_macro, roc_auc_macro = macroAvg_calc(n_classes, n_repeats, n_splits, fprTot, fpr, tpr, thr)                            \n    fpr.update(fpr_macro)\n    tpr.update(tpr_macro)\n    auc_.update(roc_auc_macro)\n\n    for i in range(n_classes):\n        if i == 0:\n            ylabel = '{} \\n True Positive Rate (TPR)'.format(clfName)\n        else:\n            ylabel = None\n        rocRpt_plotter(ax[i], classLabel, fpr[i], fpr[\"macro_{}\".format(i+1)], tpr[i], tpr[\"macro_{}\".format(i+1)], auc_[\"macro_{}\".format(i+1)], \n        auc_[\"macro_{}BestThrIdx\".format(i+1)], auc_[\"macro_{}BestThrGmeans\".format(i+1)], \n                       colors_classes[i], lw, classLabel[i], i==0, origNew, ylabel)\n    for i in range(n_classes):\n        ax[n_classes].plot(fpr[\"macro_{}\".format(i+1)], tpr[\"macro_{}\".format(i+1)],color= colors_classes[i], lw=2*lw, \n                   label='Class {0} (AUC = {1:0.2f})'.format(origNew[classLabel[i]], auc_[\"macro_{}\".format(i+1)]))        \n    ax[n_classes].plot(fpr[\"macro\"], tpr[\"macro\"], 'k', lw=2*lw, label='Macro-avg (AUC = {0:0.2f})'.format(auc_[\"macro\"]))\n    ax[n_classes].scatter(fpr[\"macro\"][auc_[\"macroBestThrIdx\"]], \n                         tpr[\"macro\"][auc_[\"macroBestThrIdx\"]], \n                         marker='o', color='k', s=2**7, zorder=20,\n                            label='Opt (GM = {0:0.2f})'.format(auc_[\"macroBestThrGmeans\"]))\n    rocAxis_plotter(ax[n_classes], lw, ylabel=None, title='Across classes')\n    fpr_models = fpr, tpr, auc_\n    plt.savefig(\"../working/{}-classROC.png\".format(n_classes))\n    return fpr_models","metadata":{"execution":{"iopub.status.busy":"2023-02-22T17:59:59.574209Z","iopub.execute_input":"2023-02-22T17:59:59.574616Z","iopub.status.idle":"2023-02-22T17:59:59.644733Z","shell.execute_reply.started":"2023-02-22T17:59:59.574584Z","shell.execute_reply":"2023-02-22T17:59:59.643599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lw = 2 # Line width\nfpr_models = []\n# for dataCleanedMergedTmp, classTmp, clfName, clf in zip(optDataCleanedMerged, optClass, optClfName, optClf):\nfor dataCleanedMergedTmp, clfName, clf in zip(optDataCleanedMerged, optClfName, optClf):\n#     fpr_model = rocTot_plotter(dataCleanedMergedTmp, reviewColName, ratingColName, ratingOrigColName, clfName, clf, lw, colors_classes)\n    fpr_model = rocTot_plotter(dataCleanedMergedTmp, clfName, clf, lw, colors_classes)\n    fpr_models.append(fpr_model)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T18:00:02.534036Z","iopub.execute_input":"2023-02-22T18:00:02.534581Z","iopub.status.idle":"2023-02-22T18:00:17.071559Z","shell.execute_reply.started":"2023-02-22T18:00:02.534543Z","shell.execute_reply":"2023-02-22T18:00:17.069292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Macro-averaged ROC comparison across multi-class scenarios, i.e., 2-class, ..., 5-class ([ToC](#TOC))<a class=\"anchor\" id=\"MacroAveragedROC\"></a> ","metadata":{}},{"cell_type":"code","source":"fprIdx, tprIdx, aucIdx = 0, 1, 2\nfig, ax = plt.subplots(1, 1, figsize=(20,5))\nfig.suptitle('ROC ({}-class; {}*{}-fold cross-validation)'.format(str(nClass), n_repeats, n_splits), #  performance evaluation technique\n            fontsize=20, fontweight='bold')  \nfor i in range(len(fpr_models)):\n    ax.plot(fpr_models[i][fprIdx][\"macro\"], fpr_models[i][tprIdx][\"macro\"], color=colors_classes[i], lw=2*lw, \n                   label='{0}-class macro-avg (AUC = {1:0.2f})'.format(nClass[i], fpr_models[i][aucIdx][\"macro\"]))        \nrocAxis_plotter(ax, lw, 'True Positive Rate (TPR)')","metadata":{"execution":{"iopub.status.busy":"2023-02-22T18:03:58.796272Z","iopub.execute_input":"2023-02-22T18:03:58.796701Z","iopub.status.idle":"2023-02-22T18:03:59.077742Z","shell.execute_reply.started":"2023-02-22T18:03:58.796666Z","shell.execute_reply":"2023-02-22T18:03:59.076463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results ([ToC](#TOC))<a class=\"anchor\" id=\"ROCResults\"></a> ","metadata":{}},{"cell_type":"code","source":"display_markdown('''\n* For each multi-class scenario, i.e., 2-class, ..., 5-class, the following procedure is repeated:\n    * Multi-class labels are binarised, i.e., One-vs-rest (OvR), and for each binary label vector (the following colour-coded ROCs) following steps are performed:\n        1. training/test dataset split using N*K-fold cross-validation (CV)\n        1. Model (pipeline: TF-IDF -> SMOTE -> fine-tuned classifier) is trained on the training dataset;\n        1. Trained model is scored on the test dataset;\n        1. [ROC](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html) curve and AUC are computed as an evaluation metric for each CV repetition;\n        1. Macro average (single black curve) is computed across CV repetitions (multiple coloured curves);\n        1. The threshold maximising the geometric mean (G-mean) of sensitivity and specificity is selected as the optimal threshold (to be used at the next step).\n    * Macro-average ROC was computed across binarised cases (last columns), which their AUC is shown below:\n     * | N-class |Macro-averaged AUC|\n       |:---|----:|\n       | **{0}**  |  **{4}**  |\n       | **{1}**| **{5}**  |\n       | **{2}**|  **{6}**  |\n       | **{3}**|  **{7}**  |\n    '''.format(nClass[0], nClass[1], nClass[2], nClass[3],\n              round(fpr_models[0][aucIdx][\"macro\"], 2), round(fpr_models[1][aucIdx][\"macro\"], 2), round(fpr_models[2][aucIdx][\"macro\"], 2),\n               round(fpr_models[3][aucIdx][\"macro\"], 2)), raw=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optimal model ([ToC](#TOC))<a class=\"anchor\" id=\"OptimalModel\"></a> \n## Confuison/Error matrix\n* The confusion matrix is a performance evaluation technique in a classification task (per-category comparison of the predicted labels to the true labels). \n* The standard/conventional confusion matrix is computed for a binary classification task (Negative and Positive labels).\n* Correctly-classified samples are either True Positive or True Negative, while the mis-classified ones are either called False Positive or False Negative.\n* Some useful evaluation metrics that can be derived from the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) are:\n * True Positive Rate (TPR), or sensitivity, or recall, or hit rate;\n * Positive Predictive Value (PPV) or precision;\n * F1 score (the harmonic mean of precision and sensitivity);\n * Accuracy (ACC). ","metadata":{}},{"cell_type":"code","source":"def confusionMatrix_plotter(ax, cf_matrix, classLabel, classLabelTot, origNew, colors_classes_CF, ylabel=None):\n    ax.axis('square')\n    cf_dim = len(cf_matrix)\n    group_percentages = ['{0:.1%}'.format(value) for value in cf_matrix.ravel() / np.sum(cf_matrix)]\n    if cf_dim == 2 and classLabel != None: #OvR\n        group_names = ['TN','FP',\"FN\", 'TP']\n        theRestTot = negClass_finder(classLabel, classLabelTot, origNew)\n        categories  = [theRestTot, origNew[classLabel]] # group_names = ['b']*(cf_dim**2)\n        labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n    else: # Multiclass\n        categories = []\n        for i in classLabelTot:\n            categories.append(origNew[i])\n        labels = group_percentages\n    labels = np.asarray(labels).reshape(cf_dim, cf_dim)        \n    cmap = colors_classes_CF\n    h = sns.heatmap(cf_matrix, ax=ax, annot = labels, cmap = cmap,fmt = '',\n                xticklabels = categories, yticklabels = categories, cbar_kws={\"shrink\": 0.5})\n    ax.set_ylim(cf_dim, 0)\n    ax.set_yticklabels(labels=ax.get_yticklabels(), va='center')    \n    if cf_dim == 2 and classLabel != None:\n        tn, fp, fn, tp = cf_matrix.ravel()\n        tpr = tp / (tp + fn)\n        ppv = tp / (tp + fp)  \n        f1 = 2 * (ppv*tpr)/(ppv+tpr)\n        acc = (tp + tn) / (tp + tn + fn + fp)\n    else: # cf_dim == n_classes:\n        acc = np.sum(np.diag(cf_matrix)) / np.sum(cf_matrix)\n        tpr = np.nan\n        ppv = np.nan \n        f1 = np.nan         \n    ax.set_xlabel(\"Predicted values\", fontdict = {'size':12}) #, labelpad = 10\n    ax.set_ylabel(ylabel, fontdict = {'size':12}) # , labelpad = 10     \n        \n    if cf_dim == 2 and classLabel != None: #classLabel != None: # Binary        \n        ax.set_title (\"Neg. labels: {0} \\nPos. label: {1}\\nTPR={2:.1%}, PPV={3:.1%}\\nF1={4:.1%}, ACC={5:.1%}\".\n                      format(theRestTot, origNew[classLabel], tpr, ppv, f1, acc), \n                                    fontdict = {'size':12}) # , pad = 20\n    else: # Multiclass\n        ax.set_title (\"Multi-class task\\nAcc={0:.2%}\".format(acc), fontdict = {'size':12})\n    return acc\n    \n\n# def split_feature_clf_confusion(X, y, test_size, clf, thr):\ndef split_feature_clf_confusion(X_train, y_train, X_test, y_test, clf, thr):\n    \"\"\"Splits into train/test; extracts features; trains & tests the classifier. Returns average test confusion matrix.\"\"\"\n#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state) # train/test split\n    model, _ = model_builder(y_train, clf, '')\n    model.fit(X_train, y_train)                 \n    if len(np.unique(y_train)) == 2: # If binary task  \n        y_score = model.predict_proba(X_test)[:, 1]\n        if thr == None:\n            thr = 0.5\n        y_score[y_score >= thr] = 1\n        y_score[y_score < thr] = 0\n    else:\n        y_score = model.predict(X_test)\n    return confusion_matrix(y_test, y_score)\n\n\n# def confusionTot_plotter(fpr_model, optDataCleanedMerged, reviewColName, ratingColName, ratingOrigColName, clfName, clf, colors_classes_CF, dataCleaned_test):\ndef confusionTot_plotter(fpr_model, optDataCleanedMerged, clfName, clf, colors_classes_CF, dataCleaned_test):    \n#     yOrig, X, y = optDataCleanedMerged[ratingOrigColName].values, optDataCleanedMerged[reviewColName].values, optDataCleanedMerged[ratingColName].values\n#     X_test, y_test, yOrig_test  = dataCleaned_test[reviewColName].values , dataCleaned_test[ratingColName].values\n    yOrig, X, y = optDataCleanedMerged[2].values, optDataCleanedMerged[0].values, optDataCleanedMerged[1].values\n    X_test, y_test, yOrig_test  = dataCleaned_test[0].values, dataCleaned_test[1].values, dataCleaned_test[2].values\n    origNew = origNewLabel_finder(yOrig, y)\n    classLabelTot = list(np.unique(y))    \n#     X_binary, Y_binary = output_binariser(X, y, classLabelTot)\n    lb = MyLabelBinarizer()\n    Y_binary = lb.fit_transform(y)\n    Y_test_binary = lb.transform(y_test)\n    \n    n_classes = len(classLabelTot)    \n#     clf = OneVsRestClassifier(clf)\n    fig, ax = plt.subplots(1, n_classes+1, figsize=(20,6)) # , sharey=True, sharex=True\n    fig.suptitle('{}-class: Confusion matrix for the optimum threshold on the ROC'.format(n_classes), fontsize=20, fontweight='bold') \n    for i, classLabel in enumerate(classLabelTot):\n#         thr = fpr_model[aucIdx][\"macro_{}BestThrVal\".format(i+1)]    \n        thr = fpr_model[\"macro_{}BestThrVal\".format(i+1)] \n#         cfMatrix = split_feature_clf_confusion(X_binary[i], Y_binary[i], test_size, clf, thr)\n        cfMatrix = split_feature_clf_confusion(X, Y_binary[:, i], X_test, Y_test_binary[:, i], clf, thr)\n        if i == 0:\n            ylabel = \"{} \\n Actual values\".format(clfName)\n        else:\n            ylabel = None \n        confusionMatrix_plotter(ax[i], cfMatrix, classLabel, classLabelTot, origNew, colors_classes_CF[i], ylabel) \n#     cfMatrix = split_feature_clf_confusion(X, y, test_size, clf, None)\n    cfMatrix = split_feature_clf_confusion(X, y, X_test, y_test, clf, None)\n    acc = confusionMatrix_plotter(ax[n_classes], cfMatrix, None, classLabelTot, origNew, 'Greys', None)         \n    plt.savefig(\"../working/{}-classConfusion.png\".format(n_classes))\n    return acc","metadata":{"execution":{"iopub.status.busy":"2023-02-22T18:11:26.545484Z","iopub.execute_input":"2023-02-22T18:11:26.545895Z","iopub.status.idle":"2023-02-22T18:11:26.571887Z","shell.execute_reply.started":"2023-02-22T18:11:26.545862Z","shell.execute_reply":"2023-02-22T18:11:26.570614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors_classes_CF = []\nfor colors_classe in colors_classes:\n    colors_classes_CF.append(sns.light_palette(colors_classe, as_cmap=True))\nacc = []\n# for fpr_model, dataCleanedMergedTmp, classTmp, clfName, clf, dataCleanedMerged_test_ in zip(fpr_models, optDataCleanedMerged, optClass, optClfName, optClf, dataCleanedMerged_test):\nfor fpr_model, dataCleanedMergedTmp, clfName, clf, dataCleanedMerged_test_ in zip(fpr_models, optDataCleanedMerged, optClfName, optClf, yMerged_test):\n#     acc_ = confusionTot_plotter(fpr_model[aucIdx], dataCleanedMergedTmp, reviewColName, ratingColName, ratingOrigColName, clfName, clf, colors_classes_CF, dataCleanedMerged_test_)\n    acc_ = confusionTot_plotter(fpr_model[aucIdx], dataCleanedMergedTmp, clfName, clf, colors_classes_CF, dataCleanedMerged_test_)\n    acc.append(acc_)","metadata":{"execution":{"iopub.status.busy":"2023-02-22T18:13:07.434240Z","iopub.execute_input":"2023-02-22T18:13:07.435107Z","iopub.status.idle":"2023-02-22T18:13:16.628080Z","shell.execute_reply.started":"2023-02-22T18:13:07.435055Z","shell.execute_reply":"2023-02-22T18:13:16.626692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results ([ToC](#TOC))<a class=\"anchor\" id=\"OptimalModelResults\"></a> ","metadata":{}},{"cell_type":"code","source":"display_markdown('''\n* For each multi-class scenario, i.e., 2-class, ..., 5-class, the following procedure is repeated:\n    * Multi-class labels are binarised, i.e., One-vs-rest (OvR), and for each binary label vector (the following colour-coded confusion matrices) following steps are performed:\n        1. Model (pipeline: TF-IDF -> SMOTE -> fine-tuned classifier) is trained on the training dataset;\n        1. Trained model is scored on the test dataset;\n        1. The scores are thresholded based on the optimal threshold obtained from the corresponding ROC;\n        1. Confusion matrix is computed as an evaluation metric.\n    * The above steps, except for the third step, i.e., scores thresholding, are also repeated for the multi-class task (the grey-coded confusion matrix), which their accuracy is shown below.\n     * | N-class |Multi-class ACC|\n       |:---|----:|\n       | **{0}**  |  **{4}**  |\n       | **{1}**| **{5}**  |\n       | **{2}**|  **{6}**  |\n       | **{3}**|  **{7}**  |\n    '''.format(nClass[0], nClass[1], nClass[2], nClass[3],\n              round(acc[0], 2), round(acc[1], 2), round(acc[2], 2), round(acc[3], 2)), raw=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References ([ToC](#TOC))<a class=\"anchor\" id=\"References\"></a> \n* [Tune Hyperparameters for Classification Machine Learning Algorithms](https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/)\n* [Tune Hyperparameters with GridSearchCV](https://www.analyticsvidhya.com/blog/2021/06/tune-hyperparameters-with-gridsearchcv/)\n* [Tuning the hyper-parameters of an estimator](https://scikit-learn.org/stable/modules/grid_search.html)\n* [Caret List of Algorithms and Tuning Parameters](https://topepo.github.io/caret/available-models.html)\n* [Python equivalent of MATLAB's \"ismember\" function](https://devpress.csdn.net/python/630459aac67703293080ba45.html)\n* [python: how to identify if a variable is an array or a scalar](https://stackoverflow.com/questions/16807011/python-how-to-identify-if-a-variable-is-an-array-or-a-scalar)\n* [MATLAB ind2sub equivalent in Python](https://stackoverflow.com/questions/28995146/matlab-ind2sub-equivalent-in-python)\n* [sklearn LabelBinarizer returns vector when there are 2 classes](https://stackoverflow.com/questions/31947140/sklearn-labelbinarizer-returns-vector-when-there-are-2-classes)\n* [A Gentle Introduction to Threshold-Moving for Imbalanced Classification](https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/)\n* [Naive Bayes Classifier in Python](https://www.kaggle.com/code/prashant111/naive-bayes-classifier-in-python)\n* [Naive Bayes Classifier From Scratch in Python](https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/)\n* [imbalanced-learn documentation](https://imbalanced-learn.org/stable/)\n* [SMOTE: Synthetic Minority Over-sampling Technique](https://www.jair.org/index.php/jair/article/view/10302)\n* [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)\n* [The optimality of Naive Bayes](https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf)\n* [Performance Comparison of Tuned and Untuned Classification Models](https://www.analyticsvidhya.com/blog/2021/06/performance-comparision-of-tuned-and-untuned-classification-models/)\n","metadata":{}}]}